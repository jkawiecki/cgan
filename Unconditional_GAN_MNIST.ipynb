{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_bGayF69SXvr"},"outputs":[],"source":["# Setup\n","import numpy as np\n","import math\n","import os\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","import torchvision.models as models\n","\n","from torch.utils.data import DataLoader\n","import torchvision.utils as vutils\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from scipy import linalg\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n","\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2bVGgzmSu88"},"outputs":[],"source":["# Global values\n","\n","n_epochs = 51\n","batch_size = 128\n","lr_d = 0.0001\n","lr_g = 0.0002\n","beta1 = 0.5\n","beta2 = 0.999\n","latent_dim = 100\n","n_classes = 10\n","\n","device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtTwbqSuRlQG"},"outputs":[],"source":["# Configure data loader\n","\n","def data_setup(dataset):\n","\n","\n","  if dataset == 'MNIST':\n","    img_size = 28\n","    channels = 1\n","    train_dataset = datasets.MNIST('data',\n","                                  train=True,\n","                                  download=True,\n","                                  transform=transforms.Compose([\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize([0.5], [0.5])\n","                                    ]))\n","  C = channels\n","  H, W = img_size, img_size\n","\n","  train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n","  print('Data loaded okay')\n","  print(f'Num batches: {len(train_loader)}')\n","\n","  batch_idx, (images, targets) = next(enumerate(train_loader))\n","\n","  plt.figure(figsize=(8,8))\n","  plt.axis(\"off\")\n","  pictures = vutils.make_grid(images[torch.randint(len(images), (100,))],nrow=10,padding=2, normalize=True)\n","  plt.imshow(pictures.permute(1,2,0))\n","  plt.show()\n","\n","  img_path = '../Unconditional'\n","\n","  if not os.path.exists(img_path):\n","    os.makedirs(img_path)\n","  save_image(pictures,fp='{}/epoch_{}-{}.png'.format(img_path, 0, 0))\n","\n","  return train_loader, C, H, W"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MGXT37MTWRy"},"outputs":[],"source":["# Generator\n","class Generator(nn.Module):\n","    # initializers\n","    def __init__(self, dataset):\n","        super(Generator, self).__init__()\n","        self.dataset = dataset\n","\n","        self.models = nn.ModuleDict({\n","             'MNIST': nn.Sequential(\n","                          nn.Linear(latent_dim, 128),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","\n","                          nn.Linear(128, 256),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(256, 512),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(512, 1024),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(1024, 1 * 28 * 28),\n","                          nn.Tanh()\n","              )\n","        })\n","\n","    # forward method\n","    def forward(self, input):\n","      x = self.models[self.dataset](input)\n","      return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcGqPuuNTZCn"},"outputs":[],"source":["# Discriminator\n","\n","class Discriminator(nn.Module):\n","    # initializers\n","    def __init__(self, dataset):\n","        super(Discriminator, self).__init__()\n","        self.dataset = dataset\n","\n","        self.models = nn.ModuleDict({\n","            'MNIST': nn.Sequential(\n","                          nn.Linear(1 * 28 * 28, 1024),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(1024, 512),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(512, 256),\n","                          nn.LeakyReLU(0.2, inplace=True),\n","                          nn.Dropout(0.3),\n","\n","                          nn.Linear(256, 1),\n","                          nn.Sigmoid()\n","            )\n","        })\n","\n","    # forward method\n","    def forward(self, input):\n","      x = self.models[self.dataset](input)\n","      return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNfNwuNCzzh9"},"outputs":[],"source":["# Inception models for FID Score\n","\n","def _inception_v3(*args, **kwargs):\n","    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n","    try:\n","        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n","    except ValueError:\n","        # Just a caution against weird version strings\n","        version = (0,)\n","\n","    # Skips default weight inititialization if supported by torchvision\n","    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n","    if version >= (0, 6):\n","        kwargs['init_weights'] = False\n","\n","    # Backwards compatibility: `weights` argument was handled by `pretrained`\n","    # argument prior to version 0.13.\n","    if version < (0, 13) and 'weights' in kwargs:\n","        if kwargs['weights'] == 'DEFAULT':\n","            kwargs['pretrained'] = True\n","        elif kwargs['weights'] is None:\n","            kwargs['pretrained'] = False\n","        else:\n","            raise ValueError(\n","                'weights=={} not supported in torchvision {}'.format(\n","                    kwargs['weights'], torchvision.__version__\n","                )\n","            )\n","        del kwargs['weights']\n","\n","    return torchvision.models.inception_v3(*args, **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1R1k1YeqQZ_k"},"outputs":[],"source":["# InceptionV3 Model for FID Score\n","\n","class InceptionV3(nn.Module):\n","    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n","\n","    # Index of default block of inception to return,\n","    # corresponds to output of final average pooling\n","    DEFAULT_BLOCK_INDEX = 3\n","\n","    # Maps feature dimensionality to their output blocks indices\n","    BLOCK_INDEX_BY_DIM = {\n","        64: 0,   # First max pooling features\n","        192: 1,  # Second max pooling featurs\n","        768: 2,  # Pre-aux classifier features\n","        2048: 3  # Final average pooling features\n","    }\n","\n","    def __init__(self,\n","                 output_blocks=[DEFAULT_BLOCK_INDEX],\n","                 resize_input=True,\n","                 normalize_input=True,\n","                 requires_grad=False):\n","\n","        super(InceptionV3, self).__init__()\n","\n","        self.resize_input = resize_input\n","        self.normalize_input = normalize_input\n","        self.output_blocks = sorted(output_blocks)\n","        self.last_needed_block = max(output_blocks)\n","\n","        assert self.last_needed_block <= 3, \\\n","            'Last possible output block index is 3'\n","\n","        self.blocks = nn.ModuleList()\n","\n","        # Using torchvision inception\n","        inception = _inception_v3(weights='DEFAULT')\n","\n","        # Block 0: input to maxpool1\n","        block0 = [\n","            inception.Conv2d_1a_3x3,\n","            inception.Conv2d_2a_3x3,\n","            inception.Conv2d_2b_3x3,\n","            nn.MaxPool2d(kernel_size=3, stride=2)\n","        ]\n","        self.blocks.append(nn.Sequential(*block0))\n","\n","        # Block 1: maxpool1 to maxpool2\n","        if self.last_needed_block >= 1:\n","            block1 = [\n","                inception.Conv2d_3b_1x1,\n","                inception.Conv2d_4a_3x3,\n","                nn.MaxPool2d(kernel_size=3, stride=2)\n","            ]\n","            self.blocks.append(nn.Sequential(*block1))\n","\n","        # Block 2: maxpool2 to aux classifier\n","        if self.last_needed_block >= 2:\n","            block2 = [\n","                inception.Mixed_5b,\n","                inception.Mixed_5c,\n","                inception.Mixed_5d,\n","                inception.Mixed_6a,\n","                inception.Mixed_6b,\n","                inception.Mixed_6c,\n","                inception.Mixed_6d,\n","                inception.Mixed_6e,\n","            ]\n","            self.blocks.append(nn.Sequential(*block2))\n","\n","        # Block 3: aux classifier to final avgpool\n","        if self.last_needed_block >= 3:\n","            block3 = [\n","                inception.Mixed_7a,\n","                inception.Mixed_7b,\n","                inception.Mixed_7c,\n","                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","            ]\n","            self.blocks.append(nn.Sequential(*block3))\n","\n","        for param in self.parameters():\n","            param.requires_grad = requires_grad\n","\n","    def forward(self, inp):\n","        \"\"\"Get Inception feature maps\n","        Parameters\n","        ----------\n","        inp : torch.autograd.Variable\n","            Input tensor of shape Bx3xHxW. Values are expected to be in\n","            range (0, 1)\n","        Returns\n","        -------\n","        List of torch.autograd.Variable, corresponding to the selected output\n","        block, sorted ascending by index\n","        \"\"\"\n","        outp = []\n","        x = inp\n","\n","        if self.resize_input:\n","            x = F.interpolate(x,\n","                              size=(299, 299),\n","                              mode='bilinear',\n","                              align_corners=False)\n","\n","        if self.normalize_input:\n","            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n","\n","        for idx, block in enumerate(self.blocks):\n","            x = block(x)\n","            if idx in self.output_blocks:\n","                outp.append(x)\n","\n","            if idx == self.last_needed_block:\n","                break\n","\n","        return outp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIm6ietIMiJ7"},"outputs":[],"source":["# FID Calculation\n","\n","def calculate_activation_statistics(images, model, batch_size=128, dims=2048, device='cpu'):\n","    model.eval()\n","    act=np.empty((len(images), dims))\n","\n","    batch = images.to(device)\n","    pred = model(batch)[0]\n","\n","        # If model output is not scalar, apply global spatial average pooling.\n","        # This happens if you choose a dimensionality not equal 2048.\n","    if pred.size(2) != 1 or pred.size(3) != 1:\n","        pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n","\n","    act = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n","\n","    mu = np.mean(act, axis=0)\n","    sigma = np.cov(act, rowvar=False)\n","    return mu, sigma\n","\n","def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    \"\"\"Numpy implementation of the Frechet Distance.\n","    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n","    and X_2 ~ N(mu_2, C_2) is\n","            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n","    \"\"\"\n","\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","\n","\n","    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = ('fid calculation produces singular product; '\n","               'adding %s to diagonal of cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","\n","\n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)\n","\n","\n","def calc_fid(real, fake, batch_size, C, H, W):\n","    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n","    model = InceptionV3([block_idx])\n","    model = model.to(device)\n","    fake = torch.reshape(fake, (batch_size, C, H, W))\n","    real = torch.reshape(real, (batch_size, C, H, W))\n","\n","    if C != 3:\n","      fake = fake.repeat(1, 3, 1, 1)\n","      real = real.repeat(1, 3, 1, 1)\n","\n","    assert(real.size() == fake.size())\n","\n","    mu_1, std_1 = calculate_activation_statistics(real, model, batch_size=batch_size, device=device)\n","    mu_2, std_2 = calculate_activation_statistics(fake, model, batch_size=batch_size, device=device)\n","\n","    fid_val = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n","    return fid_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgR8JG8jAIaV"},"outputs":[],"source":["def train(dataset, img_path):\n","\n","  # Training setup + Dataloader\n","  train_loader, C, H, W = data_setup(dataset)\n","\n","  # Loss function\n","  criterion = torch.nn.BCELoss()\n","\n","  # Initialize Generator and discriminator\n","  generator = Generator(dataset)\n","  discriminator = Discriminator(dataset)\n","\n","  generator.to(device)\n","  discriminator.to(device)\n","  criterion.to(device)\n","\n","  # Optimizers\n","  optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(beta1, beta2))\n","  optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n","\n","  print(\"Generator Parameters:\",sum(p.numel() for p in generator.parameters() if p.requires_grad))\n","  print(\"Discriminator Parameters:\",sum(p.numel() for p in discriminator.parameters() if p.requires_grad))\n","\n","\n","  g_losses_total = []\n","  d_losses_total = []\n","\n","  g_losses_epoch = []\n","  d_losses_epoch = []\n","\n","  real_ind = 0.9\n","  fake_ind = 0.1\n","  iters = 0\n","\n","\n","  for epoch in range(n_epochs):\n","\n","      for i, data in enumerate(train_loader):\n","\n","          batch_size = data[0].size(0)\n","          # Real/Fake scores\n","          real_score = torch.full((batch_size,), real_ind, device=device)\n","          fake_score = torch.full((batch_size,), fake_ind, device=device)\n","\n","          # ---------------------\n","          #  Train Discriminator\n","          # ---------------------\n","          optimizer_D.zero_grad()\n","\n","          # Forward pass for real images\n","          real_imgs = data[0].view(batch_size, H*W*C).to(device)\n","\n","          # Add noise to discriminator\n","          #real_imgs = 0.95*real_imgs + 0.05*torch.randn((real_imgs.size()), device=device)\n","\n","          real_loss = discriminator(real_imgs).view(batch_size)\n","          d_real_loss = criterion(real_loss, real_score)\n","\n","          # Forward pass for fake images\n","          noise = torch.randn(batch_size, latent_dim, device=device)\n","          gen_imgs = generator(noise)\n","          fake_loss = discriminator(gen_imgs.detach()).view(batch_size)\n","          d_fake_loss = criterion(fake_loss, fake_score)\n","\n","          # Total discriminator loss\n","          d_loss = d_real_loss + d_fake_loss\n","\n","          # Calculate gradients\n","          d_loss.backward()\n","\n","          # Update D\n","          optimizer_D.step()\n","\n","\n","          # -----------------\n","          #  Train Generator\n","          # -----------------\n","          optimizer_G.zero_grad()\n","\n","          # Add noise to discriminator\n","          #gen_imgs = 0.95*gen_imgs + 0.05*torch.randn((gen_imgs.size()), device=device)\n","\n","          output = discriminator(gen_imgs)\n","          g_loss = criterion(output.squeeze(), real_score)\n","\n","          g_loss.backward()\n","          optimizer_G.step()\n","\n","\n","          d_losses_total.append(d_loss.item())\n","          g_losses_total.append(g_loss.item())\n","          iters += 1\n","\n","\n","      # Each Epoch\n","      d_losses_epoch.append(d_loss.item())\n","      g_losses_epoch.append(g_loss.item())\n","      fid = calc_fid(real_imgs, gen_imgs, batch_size, C, H, W)\n","\n","      print (\"[Epoch %d/%d] \\t[Batch %d/%d] \\tF[D loss: %.4f] \\tF[G loss: %.4f] \\tFID: %.4f\" % (epoch, n_epochs, i+1, len(train_loader), d_loss.item(), g_loss.item(), fid))\n","\n","      # Every 5 epochs generate sample image\n","      if (epoch % 5 == 0):\n","        print(f\"Epochs completed: {epoch}\\t Iterations completed: {iters}\")\n","\n","        with torch.no_grad():\n","          noise = torch.randn(batch_size, latent_dim, device=device)\n","          gen_imgs = generator(noise).cpu().view(batch_size, C, H, W)\n","\n","          plt.figure(figsize=(8,8))\n","          plt.axis(\"off\")\n","          pictures = vutils.make_grid(gen_imgs[torch.randint(len(gen_imgs), (100,))],nrow=10,padding=2, normalize=True)\n","          plt.imshow(pictures.permute(1,2,0))\n","          if not os.path.exists(img_path):\n","            os.makedirs(img_path)\n","          save_image(pictures,fp='{}/epoch_{}-{}.png'.format(img_path, epoch, n_epochs))\n","\n","          plt.show()\n","\n","  return d_losses_total, g_losses_total, d_losses_epoch, g_losses_epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdV7vGabIJgE"},"outputs":[],"source":["# Plot loss\n","def plot_total(d_losses_total, g_losses_total):\n","  plt.figure(figsize=(10,5))\n","  plt.title(\"Generator and Discriminator Loss During Training (total)\")\n","  plt.plot(g_losses_total,label=\"G\")\n","  plt.plot(d_losses_total,label=\"D\")\n","  plt.xlabel(\"Iterations\")\n","  plt.ylabel(\"Loss\")\n","  plt.legend()\n","  plt.show()\n","\n","def plot_epoch(d_losses_epoch, g_losses_epoch):\n","  plt.figure(figsize=(10,5))\n","  plt.title(\"Generator and Discriminator Loss During Training (epoch)\")\n","  plt.plot(g_losses_epoch,label=\"G\")\n","  plt.plot(d_losses_epoch,label=\"D\")\n","\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Loss\")\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"code","source":["# Training MNIST\n","\n","dataset = 'MNIST'\n","\n","img_path = '../Unconditional'\n","\n","d_losses_total_mnist, g_losses_total_mnist, d_losses_epoch_mnist, g_losses_epoch_mnist = train(dataset, img_path)\n","\n","plot_total(d_losses_total_mnist, g_losses_total_mnist)\n","plot_epoch(d_losses_epoch_mnist, g_losses_epoch_mnist)"],"metadata":{"id":"if4CrYVtMS7M"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}